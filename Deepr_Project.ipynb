{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oZUDi1ufBsNu"},"outputs":[],"source":["BoW+LR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4737,"status":"ok","timestamp":1683258483995,"user":{"displayName":"Sanjib Kumar Panigrahi","userId":"08841323902198361382"},"user_tz":420},"id":"mZlCiCKuBXQz","outputId":"44189ce2-1ff3-42ec-92d6-ff65393eb061"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.552\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load the UCI Diabetes dataset\n","#data = pd.read_csv(\"diabetic_data.csv\", usecols=[\"readmitted\", \"diag_1\", \"diag_2\", \"diag_3\"])\n","\n","# Replace missing values with the string \"missing\"\n","data.fillna(\"missing\", inplace=True)\n","\n","# Create a bag-of-words representation of the diagnosis codes\n","vectorizer = CountVectorizer(token_pattern=\"[0-9]+\")\n","X = vectorizer.fit_transform(data[[\"diag_1\", \"diag_2\", \"diag_3\"]].apply(lambda x: \" \".join(x), axis=1))\n","\n","# Use logistic regression to predict unplanned readmissions\n","lr = LogisticRegression()\n","lr.fit(X, data[\"readmitted\"])\n","\n","# Calculate accuracy on the training data\n","y_pred = lr.predict(X)\n","accuracy = accuracy_score(data[\"readmitted\"], y_pred)\n","print(\"Accuracy: {:.3f}\".format(accuracy))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGE44rdfBo49"},"outputs":[],"source":["Deepr (Rand-init)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxRyuXBO9KxS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90820,"status":"ok","timestamp":1683257872053,"user":{"displayName":"Sanjib Kumar Panigrahi","userId":"08841323902198361382"},"user_tz":420},"id":"gzKco0Ig9LNp","outputId":"6abe78d9-c0ce-4859-f0e4-9aa3cb6e01e7"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-2-eaec9abebf37>:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data[col] = label_encoder.fit_transform(data[col])\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","613/613 [==============================] - 3s 3ms/step - loss: 0.9424 - accuracy: 0.5444 - val_loss: 0.9101 - val_accuracy: 0.5638\n","Epoch 2/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.9157 - accuracy: 0.5651 - val_loss: 0.9050 - val_accuracy: 0.5678\n","Epoch 3/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.9093 - accuracy: 0.5690 - val_loss: 0.9033 - val_accuracy: 0.5691\n","Epoch 4/50\n","613/613 [==============================] - 2s 4ms/step - loss: 0.9054 - accuracy: 0.5709 - val_loss: 0.9007 - val_accuracy: 0.5723\n","Epoch 5/50\n","613/613 [==============================] - 2s 4ms/step - loss: 0.9029 - accuracy: 0.5734 - val_loss: 0.9008 - val_accuracy: 0.5724\n","Epoch 6/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.9010 - accuracy: 0.5749 - val_loss: 0.8996 - val_accuracy: 0.5730\n","Epoch 7/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.9004 - accuracy: 0.5750 - val_loss: 0.8987 - val_accuracy: 0.5721\n","Epoch 8/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8986 - accuracy: 0.5766 - val_loss: 0.9005 - val_accuracy: 0.5716\n","Epoch 9/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8973 - accuracy: 0.5763 - val_loss: 0.8976 - val_accuracy: 0.5724\n","Epoch 10/50\n","613/613 [==============================] - 3s 5ms/step - loss: 0.8980 - accuracy: 0.5762 - val_loss: 0.8967 - val_accuracy: 0.5717\n","Epoch 11/50\n","613/613 [==============================] - 3s 4ms/step - loss: 0.8971 - accuracy: 0.5770 - val_loss: 0.8976 - val_accuracy: 0.5728\n","Epoch 12/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8955 - accuracy: 0.5770 - val_loss: 0.8969 - val_accuracy: 0.5727\n","Epoch 13/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8949 - accuracy: 0.5778 - val_loss: 0.8963 - val_accuracy: 0.5712\n","Epoch 14/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8941 - accuracy: 0.5778 - val_loss: 0.8956 - val_accuracy: 0.5720\n","Epoch 15/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8944 - accuracy: 0.5774 - val_loss: 0.8966 - val_accuracy: 0.5717\n","Epoch 16/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8942 - accuracy: 0.5772 - val_loss: 0.8958 - val_accuracy: 0.5741\n","Epoch 17/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8931 - accuracy: 0.5788 - val_loss: 0.8952 - val_accuracy: 0.5730\n","Epoch 18/50\n","613/613 [==============================] - 2s 4ms/step - loss: 0.8927 - accuracy: 0.5790 - val_loss: 0.8946 - val_accuracy: 0.5743\n","Epoch 19/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8918 - accuracy: 0.5792 - val_loss: 0.8947 - val_accuracy: 0.5750\n","Epoch 20/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8903 - accuracy: 0.5796 - val_loss: 0.8940 - val_accuracy: 0.5724\n","Epoch 21/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8903 - accuracy: 0.5823 - val_loss: 0.8936 - val_accuracy: 0.5735\n","Epoch 22/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8897 - accuracy: 0.5817 - val_loss: 0.8931 - val_accuracy: 0.5736\n","Epoch 23/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8886 - accuracy: 0.5817 - val_loss: 0.8927 - val_accuracy: 0.5748\n","Epoch 24/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8877 - accuracy: 0.5820 - val_loss: 0.8921 - val_accuracy: 0.5744\n","Epoch 25/50\n","613/613 [==============================] - 3s 5ms/step - loss: 0.8882 - accuracy: 0.5809 - val_loss: 0.8918 - val_accuracy: 0.5747\n","Epoch 26/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8878 - accuracy: 0.5812 - val_loss: 0.8914 - val_accuracy: 0.5740\n","Epoch 27/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8862 - accuracy: 0.5841 - val_loss: 0.8916 - val_accuracy: 0.5732\n","Epoch 28/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8863 - accuracy: 0.5825 - val_loss: 0.8911 - val_accuracy: 0.5732\n","Epoch 29/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8863 - accuracy: 0.5825 - val_loss: 0.8914 - val_accuracy: 0.5739\n","Epoch 30/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8856 - accuracy: 0.5831 - val_loss: 0.8905 - val_accuracy: 0.5737\n","Epoch 31/50\n","613/613 [==============================] - 2s 4ms/step - loss: 0.8848 - accuracy: 0.5827 - val_loss: 0.8898 - val_accuracy: 0.5736\n","Epoch 32/50\n","613/613 [==============================] - 2s 4ms/step - loss: 0.8847 - accuracy: 0.5844 - val_loss: 0.8898 - val_accuracy: 0.5753\n","Epoch 33/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8843 - accuracy: 0.5844 - val_loss: 0.8900 - val_accuracy: 0.5745\n","Epoch 34/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8841 - accuracy: 0.5831 - val_loss: 0.8896 - val_accuracy: 0.5752\n","Epoch 35/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8829 - accuracy: 0.5845 - val_loss: 0.8897 - val_accuracy: 0.5745\n","Epoch 36/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8827 - accuracy: 0.5823 - val_loss: 0.8881 - val_accuracy: 0.5742\n","Epoch 37/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8826 - accuracy: 0.5846 - val_loss: 0.8881 - val_accuracy: 0.5769\n","Epoch 38/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8819 - accuracy: 0.5820 - val_loss: 0.8888 - val_accuracy: 0.5741\n","Epoch 39/50\n","613/613 [==============================] - 2s 4ms/step - loss: 0.8814 - accuracy: 0.5846 - val_loss: 0.8887 - val_accuracy: 0.5748\n","Epoch 40/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8816 - accuracy: 0.5830 - val_loss: 0.8885 - val_accuracy: 0.5748\n","Epoch 41/50\n","613/613 [==============================] - 2s 3ms/step - loss: 0.8812 - accuracy: 0.5856 - val_loss: 0.8884 - val_accuracy: 0.5741\n","Epoch 41: early stopping\n","613/613 [==============================] - 1s 1ms/step\n","Accuracy: 0.5740655754423538\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","\n","# Load the data\n","data = pd.read_csv('diabetic_data.csv')\n","columns_to_remove = ['encounter_id', 'patient_nbr', 'weight', 'payer_code', 'medical_specialty']\n","data = data.drop(columns_to_remove, axis=1)\n","# Preprocess the data\n","data = data.replace('?', np.nan)\n","data = data.dropna()\n","\n","# Convert categorical features to numerical using label encoding\n","cat_cols = data.select_dtypes(include=['object']).columns\n","label_encoder = LabelEncoder()\n","for col in cat_cols:\n","    data[col] = label_encoder.fit_transform(data[col])\n","\n","# Split into X and y\n","X = data.drop('readmitted', axis=1)\n","y = data['readmitted']\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","# Scale the numerical features\n","num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n","scaler = StandardScaler()\n","X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n","X_test[num_cols] = scaler.transform(X_test[num_cols])\n","\n","# Define the model\n","model = Sequential()\n","model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(3, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","\n","# Train the model\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n","history = model.fit(X_train, to_categorical(y_train), epochs=50, batch_size=128, \n","                    validation_data=(X_test, to_categorical(y_test)), callbacks=[early_stopping])\n","\n","y_pred = model.predict(X_test)\n","y_pred_class = np.argmax(y_pred, axis=1)\n","accuracy = accuracy_score(y_test, y_pred_class)\n","precision = precision_score(y_test, y_pred_class, average='weighted')\n","recall = recall_score(y_test, y_pred_class, average='weighted')\n","confusion = confusion_matrix(y_test, y_pred_class)\n","print('Accuracy:', accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX63fMhqB4Na"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46935,"status":"ok","timestamp":1683427122630,"user":{"displayName":"Sanjib Kumar Panigrahi","userId":"08841323902198361382"},"user_tz":420},"id":"dmWXAy-zB4wz","outputId":"60063950-9c38-4b2f-da31-efd9723bdb43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","1026/1026 [==============================] - 7s 4ms/step - loss: 0.5635 - accuracy: 0.7564 - val_loss: 0.5508 - val_accuracy: 0.7609\n","Epoch 2/10\n","1026/1026 [==============================] - 4s 3ms/step - loss: 0.5565 - accuracy: 0.7577 - val_loss: 0.5500 - val_accuracy: 0.7609\n","Epoch 3/10\n","1026/1026 [==============================] - 4s 3ms/step - loss: 0.5551 - accuracy: 0.7576 - val_loss: 0.5499 - val_accuracy: 0.7609\n","Epoch 4/10\n","1026/1026 [==============================] - 5s 4ms/step - loss: 0.5543 - accuracy: 0.7575 - val_loss: 0.5489 - val_accuracy: 0.7609\n","Epoch 5/10\n","1026/1026 [==============================] - 4s 4ms/step - loss: 0.5546 - accuracy: 0.7576 - val_loss: 0.5492 - val_accuracy: 0.7609\n","Epoch 6/10\n","1026/1026 [==============================] - 4s 4ms/step - loss: 0.5543 - accuracy: 0.7577 - val_loss: 0.5489 - val_accuracy: 0.7609\n","Epoch 7/10\n","1026/1026 [==============================] - 4s 4ms/step - loss: 0.5538 - accuracy: 0.7575 - val_loss: 0.5490 - val_accuracy: 0.7609\n","Epoch 8/10\n","1026/1026 [==============================] - 4s 4ms/step - loss: 0.5541 - accuracy: 0.7576 - val_loss: 0.5504 - val_accuracy: 0.7609\n","Epoch 9/10\n","1026/1026 [==============================] - 4s 4ms/step - loss: 0.5547 - accuracy: 0.7575 - val_loss: 0.5493 - val_accuracy: 0.7609\n","Epoch 10/10\n","1026/1026 [==============================] - 4s 4ms/step - loss: 0.5540 - accuracy: 0.7576 - val_loss: 0.5493 - val_accuracy: 0.7609\n","220/220 - 1s - loss: 0.5552 - accuracy: 0.7557 - 500ms/epoch - 2ms/step\n","Test accuracy: 0.7556503415107727\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load the dataset\n","df = pd.read_csv(\"diabetic_data.csv\")\n","\n","# Preprocess the dataset\n","df = df[df[\"readmitted\"].isin([\"<30\", \">30\"])]\n","df[\"readmitted\"] = (df[\"readmitted\"] == \"<30\").astype(int)\n","df = df[[\"diag_1\", \"diag_2\", \"diag_3\", \"readmitted\"]]\n","df = df.dropna()\n","df[\"diagnoses\"] = df[\"diag_1\"] + \" \" + df[\"diag_2\"] + \" \" + df[\"diag_3\"]\n","df = df[[\"diagnoses\", \"readmitted\"]]\n","\n","# Split the dataset into training, validation, and test sets\n","train_df = df.sample(frac=0.7, random_state=42)\n","val_df = df.drop(train_df.index).sample(frac=0.5, random_state=42)\n","test_df = df.drop(train_df.index).drop(val_df.index)\n","\n","# Train a Word2Vec model on the diagnoses\n","sentences = [s.split() for s in train_df[\"diagnoses\"].values]\n","w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n","\n","# Define the tokenizer and vocabulary size\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df[\"diagnoses\"].values)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Define the maximum sequence length\n","max_seq_len = max([len(s.split()) for s in train_df[\"diagnoses\"].values])\n","\n","# Define the embedding matrix\n","embedding_dim = 100\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in tokenizer.word_index.items():\n","    if word in w2v_model.wv.key_to_index:\n","      embedding_matrix[i] = w2v_model.wv[word]\n","\n","# Convert the sentences to sequences of word indices\n","train_sequences = tokenizer.texts_to_sequences(train_df[\"diagnoses\"].values)\n","val_sequences = tokenizer.texts_to_sequences(val_df[\"diagnoses\"].values)\n","test_sequences = tokenizer.texts_to_sequences(test_df[\"diagnoses\"].values)\n","\n","# Pad the sequences to have the same length\n","train_sentences = pad_sequences(train_sequences, maxlen=max_seq_len, padding=\"post\", truncating=\"post\")\n","val_sentences = pad_sequences(val_sequences, maxlen=max_seq_len, padding=\"post\", truncating=\"post\")\n","test_sentences = pad_sequences(test_sequences, maxlen=max_seq_len, padding=\"post\", truncating=\"post\")\n","\n","# Define the model architecture\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False),\n","    tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"same\"),\n","    tf.keras.layers.MaxPooling1D(pool_size=2),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation=\"relu\"),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","# Compile the model\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","# Train the model\n","# Train the model\n","batch_size = 32\n","epochs = 10\n","model.fit(train_sentences, train_df[\"readmitted\"], batch_size=batch_size, epochs=epochs, validation_data=(val_sentences, val_df[\"readmitted\"]))\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_sentences, test_df[\"readmitted\"], verbose=2)\n","print(f\"Test accuracy: {test_acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRRdqVhV6wmX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFX86PjL6wkB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtjCGYBY6whr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bi2jqfx6wei"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
